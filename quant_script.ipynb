{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXYwGvu6yKNffyKKx1kjKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LennyTheIntern/valorant_tflite_model/blob/master/quant_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hg06hWj1NkT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hkdglABp-AS"
      },
      "source": [
        "import tensorflow as tf\n",
        "saved_model_dir  ='Tensorflow/workspace/models/my_ssd_mobnet/tfliteexport/saved_model'\n",
        "tflite_model_quant_file = 'tpu_detect.tflite'\n",
        "img_dir = 'Tensorflow/workspace/images/train/'\n",
        "trainrec = 'Tensorflow/workspace/annotations/train.record'\n",
        "testrec = 'Tensorflow/workspace/annotations/test.record'\n",
        "\n",
        "def data_gen():\n",
        "    image_arr = []\n",
        "    for i in range(160):\n",
        "        inst = anns[i]\n",
        "        file = inst['filename']\n",
        "        img = cv2.imread(img_dir + file)\n",
        "        img = cv2.resize(img, (NORM_H, NORM_W)) # height and width of the image\n",
        "        img = img / 255.0\n",
        "        img = img.astype(np.float32)  \n",
        "        a.append(img)\n",
        "    image_arr = np.array(a)\n",
        "    print(a.shape) # a is np array of 160 3D images\n",
        "    img = tf.data.Dataset.from_tensor_slices(image_arr).batch(1) # the more files you have the larger the batch size should be\n",
        "    for i in img.take(BATCH_SIZE):\n",
        "        print(i)\n",
        "        yield [i]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "tflite_quant_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygC9FPGD1BFG"
      },
      "source": [
        "!edgetpu_compiler tpu_detect.tflite"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}